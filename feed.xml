<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://notnanton.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://notnanton.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-24T17:28:45+00:00</updated><id>https://notnanton.github.io/feed.xml</id><title type="html">blank</title><subtitle>A page displaying Anton&apos;s work, goals, and aspirations. </subtitle><entry><title type="html">My PhD Vision</title><link href="https://notnanton.github.io/blog/2024/phd_vision/" rel="alternate" type="text/html" title="My PhD Vision"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://notnanton.github.io/blog/2024/phd_vision</id><content type="html" xml:base="https://notnanton.github.io/blog/2024/phd_vision/"><![CDATA[<h2 id="overview">Overview</h2> <h3 id="goal">Goal</h3> <p>An intelligent agent in direct interaction and quick in-context learning.</p> <h3 id="strategy">Strategy</h3> <p>Set up cognitive architecture with plan to train it and evolve it. For PhD: Focus on aspects and study composites in projects.</p> <p>Concrete Plan:</p> <ol> <li>Get suitable testing ground (robot with good simulation, decent grid world)</li> <li>Make overview of AGI architectures/plans (Sutton’s Alberta Plan, Schmidhuber’s Big Net, Le Cun’s JEPA, etc). Write paper summarizing key ingredients, distinctions.</li> <li>Make own plan and describe in detail how modules interact.</li> <li>Work on most promising module. Current promise: meta-learning with adapters and MoE.</li> <li>Select up to 3 modules that can be researched on during the PhD and publish them along with the conjoining architectural plan.</li> </ol> <h2 id="cognitive-architecture">Cognitive Architecture</h2> <h3 id="concept">Concept</h3> <p>We define an overall cognitive architecture that achieves the goal. A figure will show up here, illustrating the relationships of the modules.</p> <h3 id="individual-modules-and-neurological-analogies">Individual Modules and Neurological Analogies</h3> <ul> <li>Policy network(s) - division of network of policy networks responsible for actuator regions but overall connectedness</li> <li>Value(feeling) network - output of multi-faceted reward, analogy to control of endorphins</li> <li>Intrinsic motivation network(s) - output of rewards for curiosity, pain-avoidance, beauty-seeking, death avoidance.</li> <li>Perception Network - produces input of policy network by simulating real world, applies sensory biases to filter out noise and generates internal coherent, multi-modal integration of all senses. Very robust, changes only at short beginning of life and evolutionary.</li> <li>Affordance network - (possibly integrated as a conditional mode within the perception network.) Takes in state and intention, produces projected action/series of states/world flow.</li> <li>Word model network - strongly connected to possible affordance network. Produces next state</li> </ul> <h3 id="learning-hierarchies">Learning Hierarchies</h3> <p>Learning at different levels:</p> <ol> <li><strong>Evolutionary</strong>: defines base reactive behavior, main cognitive biases, (basic physical “laws”), architectural improvements. Evolutionary algorithm, guided by epi-genetic variations (e.g. network node stability/importance influences allowed change of weights - See Uber Paper)</li> <li><strong>Lifelong</strong>: integration of facts, long-term memory, core behavior and automatism. Policy network is trained - maybe by meta-learning over situational episodes.</li> <li><strong>Situational</strong>: Quick adaption using adapters (LoRAs). Adapters need to be integrateable into network if they are useful enough.</li> </ol> <h2 id="project-ideas">Project ideas</h2> <p>My <strong>awesome</strong> project ideas in <em>RL</em> and <em>meta-learning</em>.</p> <details><summary>Project 1</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details> <p>(Legend RL=reinforcement-learning, DL=Deep Learning)</p> <ol> <li>RL Projects: ⋅⋅* defining architecture that can think, plan, adapt ⋅⋅* incorporate knowledge of CLIP or LLMs into agent, using them either as knowledge base, its token embeddings as latent space to formulate thoughts, or as generalized algorithms that can be abused in contexts where no language is necessary ⋅⋅* RL: put planning steps (interaction path of world model + policy) into LSTM memory to let it “think”, then plan new paths and finally decide after planning is done. ⋅⋅* RL: Hierarchical actor-critic on any domain using quantized/symbolized (VQ-VAE - like) options/sub-tasks ⋅⋅* ?RL + Meta Learning: define learnability via meta-learning world model</li> <li>Meta learning Projects</li> <li>Other Projcets ⋅⋅* Meta-learning: divide dataset in clusters, train neural net using MAML on it, clusters as separate “skills”, increase granularity of clusters over training time</li> <li>Deep Learning Projects ⋅⋅* DL: learning a training schedule akin to prioritized replay but also focusing on: which samples to reject entirely? Which ones combine well with which other ones to get a clean gradient? ⋅⋅* DL: training very large models with mostly frozen weights and only train adapters to modify flow ⋅⋅* DL: train music or video model ⋅⋅* DL: live-reacting plazmapunk</li> </ol>]]></content><author><name>Anton Wiehe</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[A short overview of my PhD PLan]]></summary></entry></feed>